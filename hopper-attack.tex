\section{le-ROD security of the [HvAL] \texttt{One-Bit} Scheme}

Here we consider the \texttt{One-bit} scheme of Hopper et al.\
\cite{HvAL}, and show that it is somewhat fragile with respect to
our XXX-security notion.

\begin{figure}[tph]
\begin{center}
\hfpages{.4}
{
\underline{$\enc^H_K(m;(h,N))$}\\[2pt]
for $i=1$ to~$\ell$\\
\nudge $s_i \getsr \mathcal{C}_h$\\
\nudge $s'_i \getsr \mathcal{C}_h$\\
\nudge if $H(K \concat \langle N+i \rangle \concat s_i)=m$ then $c_i \gets s_i$\\
\nudge else $c_i \gets s'_i$\\
\nudge $h \gets h \concat c_i$\\
$N \gets N + \ell$\\
return $((h,N), c_1 c_2 \cdots c_\ell)$

\medskip
\underline{$\dec^H_K(c_1 c_2 \cdots c_\ell;(h,N))$}\\[2pt]
if $\sum_{i=1}^{\ell} H(K \concat \langle N+i \rangle \concat
c_i) > \ell/2$ then $m \gets 1$\\
else $m \gets 0$\\
$N \gets N+\ell$\\
return $((h,N),m)$
}
{
\underline{Adversary~$\advA_1^H(c_1 c_2 \cdots c_\ell, K)$}\\[2pt]
$t \gets \sum_{i=0}^{\ell} H(K \concat \langle i \rangle \concat c_i)$\\
if $\left| t - \frac{\ell}{2} \right| \geq \delta\frac{\ell}{2}$ 
 then return 1\\
return 0

\medskip
\underline{Adversary~$\advA_2^H(c_1 c_2 \cdots c_\ell)$}\\[2pt]
for $K \in \mathcal{K}$\\
%\nudge $t \gets \sum_{i=1}^{\ell} H(K \concat \langle i \rangle \concat c_i)$\\
%\nudge if $\left| t - \frac{\ell}{2} \right| \geq \delta\frac{\ell}{2}$ 
\nudge if $\advA_1(c_1 c_2 \cdots c_\ell, K) \Rightarrow 1$ then return 1\\
return 0
}
\caption{Hopper et al.\ \texttt{One-Bit} Scheme with the PRF
  $F_K(x)=H(K \concat x)$, and the attacks.  [Note: can probably
  extend this to arbitrary PRFs by changing the threshold for
  returning 1 in~$\advA_2$
  appropriately... since if any non-negligible fraction~$\alpha$ of keys result
  in ``non-random'' behavior, you won't have a good PRF to start
  with.]  Adversary~$\advA_1$ is for known-key settings,
  while~$\advA_2$ is for (low-entropy) unknown-key settings.  For
  simplicity, we assume that $N=0$ initially, and that $\ell$ is
  even.  The parameter~$\delta$ is set to $1/8$ in the theorem
  statements. 
\textcolor{cyan}{Alternative $\advA_2$ is to mount a key-recovery attack, e.g.\ finding the key that maximizes the distance between~$t$ and~$\ell/2$.}}
\label{fig:one-bit}
\end{center}
\end{figure}

\begin{theorem}(Informal.)\rm Assume that $\forall i\in[q]$ and $h_{i-1}$, 
the distributions $\mathcal{C}_{h_{i-1}}$ have min-entropy of at least one. 
When~$H\colon\bits^* \to \bits$ is a random oracle, the \texttt{One-Bit} scheme $(\enc^H,\dec^H)$ in
Figure~\ref{fig:one-bit} is not \leROD-secure.  In particular, setting $\delta=1/8$,
adversary~$\advA_1$ achieves advantage at least 
\[
1- \left(e^{-\frac{\ell}{288}}\mdist(0) + e^{-\frac{\ell}{320}}\mdist(1) + 2e^{-\frac{\ell}{128}} \right)\,.
\] 
When $\ell \geq 256$, then, the advantage is at least $1-0.412\mdist(0)-0.450\mdist(1)-0.271$.
\end{theorem}
\tsnote{How does this value of~$\ell$ compare to what you need for good correctness?  Seems likely to be larger that what you need for the same level of correctness, since the distributions in the correctness setting are farther apart.}
\begin{proof}(\textit{Still needs polishing.})
We begin by considering the case that~$b=0$, so that $c_1 c_2 \cdots c_\ell$ resulted from
independent samples from the distribution determined by
$\mathcal{C}_h$, where~$h$ takes on whatever is its initial value (and
can be ignored for the attack).  Let $X_i = H(K \concat \langle i
\rangle \concat c_i)$, so that $\Prob{X_i = 1}=\Prob{X_i=0}=1/2$ for
all $i \in [\ell]$, and $t = \sum_{i=1}^{\ell} X_i$ is a Binomial
random varible with $\mu=\mathbb{E}[t]=\ell/2$.  In this case, a standard Chernoff
bound [[MU], Corrolaries 4.9 and 4.10] gives, for $0 < \delta < 1$
\[
\Prob{\advA_1 \Rightarrow 1 \given b=0} = \Prob{\left| t - \frac{\ell}{2} \right| \geq \delta \frac{\ell}{2}} \leq 2e^{-\frac{\ell\delta^2}{2}}
\]
We will fix a value for~$\delta$ in a moment.

Now consider the case that~$b=1$, with message~$m=0$, so that $c_1 c_2 \cdots c_\ell$ are
the result of~$\enc^H_K(0)$.  Recall that, at the time that $c_i$ was
assigned, it was to one of two independent samples $s_i,s'_i$ from the
channel distribution.  In particular, it was assigned the value of~$s$
iff $H(K \concat \langle i \rangle \concat s)=0$.  Thus, letting $X_i = H(K
\concat \langle i \rangle \concat c_i)$, we have 
\begin{align*}
\Prob{X_i=0} =& 
\, \Prob{X_i=0 \cap c_i=s_i} + \Prob{X_i=0 \cap c_i=s'_i \cap (s_i=s'_i)} + 
    \Prob{X_i=0 \cap c_i=s'_i \cap (s_i \neq s'_i)}\\
=&\,  \Prob{c_i=s_i} + \Prob{X_i=0 \cap c_i=s'_i \cap (s_i \neq s'_i)}\\
=&\, \Prob{H(K \concat \langle i \rangle \concat s)=0} 
    + \Prob{X_i=0 \cap c_i=s'_i \given (s_i \neq s'_i)}\Prob{s_i \neq s'_i}\\
=&\, \frac{1}{2} + \Prob{H(K \concat \langle i \rangle \concat s_i)=1 \cap H(K
  \concat \langle i \rangle \concat s'_i)=0} \Prob{s_i \neq s'_i} \\
=&\, \frac{1}{2} + \frac{1}{4} \Prob{s_i \neq s'_i} \\
=&\, \frac{1}{2} + \frac{1}{4} (1-\kappa(\mathcal{C}_{h_{i-1}}))
\end{align*}
where $h_{i-1}$ is the history at the time that $s_i,s'_i$ are
sampled, and $\kappa(\mathcal{C}_h)$ is the collision probability
of the indicated distribution.  Thus, when the message was $m=0$, we have 
$
\mu_0=\sum_{i=1}^{\ell}
\mathbb{E}[X_i] = \mu - \frac{1}{4}\sum_{i=1}^{\ell}(1-\kappa(\mathcal{C}_{h_{i-1}}))
$.  
By a symmetrical argument, when $b=1$ and the message was $m=1$, we have
$\Prob{X_i=1} = \frac{1}{2} + \frac{1}{4}
(1-\kappa(\mathcal{C}_{h_{i-1}}))$, and $
\mu_1=\sum_{i=1}^{\ell}
\mathbb{E}[X_i] = \mu + \frac{1}{4}\sum_{i=1}^{\ell}(1-\kappa(\mathcal{C}_{h_{i-1}}))
$.  

Let $\gamma = \frac{1}{4}\sum_{i=1}^{\ell}(1-\kappa(\mathcal{C}_{h_{i-1}}))$, and let
$ \beta_{i-1} = -\log \max_{s}\Prob{S\getsr\mathcal{C}_{h_{i-1}}\colon
  S=s}$.
By standard results relating the min-entropy and the
R{\'e}nyi-entropy, it follows that $2^{-2\beta_{i-1}} \leq
\kappa(C_{h_{i-1}}) \leq 2^{-\beta_{i-1}}$ for all $i\in[\ell]$.
Thus,
\[
\frac{1}{4}\sum_{i=1}^{\ell}\left(1-\frac{1}{2^{\beta_{i-1}}} \right)
\leq \gamma \leq \frac{1}{4}\sum_{i=1}^{\ell} \left(1-\frac{1}{2^{2\beta_{i-1}}} \right)
\]
which leads to upper- and lowerbounds on $\mu_0$ and $\mu_1$ in terms of $\mu$ and the min-entropies
\begin{align*}
\mu - \frac{1}{4}\sum_{i=1}^{\ell} \left(1-\frac{1}{2^{2\beta_{i-1}}} \right) \leq
\mu_0 \leq 
\mu - \frac{1}{4}\sum_{i=1}^{\ell} \left(1-\frac{1}{2^{\beta_{i-1}}} \right) \\
\mu + \frac{1}{4}\sum_{i=1}^{\ell}\left(1-\frac{1}{2^{\beta_{i-1}}} \right)  
\leq \mu_1 \leq 
\mu + \frac{1}{4}\sum_{i=1}^{\ell}\left(1-\frac{1}{2^{2\beta_{i-1}}} \right) 
\end{align*}
Recall that $\mu=\ell/2$, and an assumption of the theorem is that 
$\forall i\in [q], \beta_{i-1} \geq 1$.  Since the $\beta_{i-1}$ are also finite, we have
$\frac{\ell}{4} < \mu_0 \leq \frac{3\ell}{8}$, and 
$\frac{5\ell}{8} \leq \mu_1 < \frac{3\ell}{4}$.
\noindent
Thus, in the case that $b=1$, we have
\begin{align*}
\Prob{\advA_1 \Rightarrow 0} =&\; \Prob{\advA_1 \Rightarrow 0 | m=0}\Prob{m=0} + \Prob{\advA_1 \Rightarrow 0 | m=1}\Prob{m=1}\\
< &\;
\Prob{\left. \sum_{i=1}^{\ell} X_i > (1-\delta)\frac{\ell}{2} \;\right| m=0}\mdist(0) 
+ \Prob{\left. \sum_{i=1}^{\ell} X_i < (1+\delta)\frac{\ell}{2} \; \right| m=1}\mdist(1)\\
< &\;
\Prob{\left. \sum_{i=1}^{\ell} X_i > \frac{7\ell}{16} \;\right| m=0}\mdist(0) 
+ \Prob{\left. \sum_{i=1}^{\ell} X_i < \frac{9\ell}{16} \;\right| m=1 }\mdist(1)
\end{align*}
where the last line is obtained by setting $\delta=1/8$. (We choose $\delta=1/8$ because $7\ell/16$ is the midway point between $3\ell/8$, the upperbound on $\mu_0$, and $\ell/2$; likewise $9\ell/16$ is midway between $\ell/2$ and $5\ell/8$.)  Since $\advA_1$ is more likely to output 0 when $\mu_0,\mu_1$ are closer to~$\mu$, we rewrite $7\ell/16$ and $9\ell/16$ in the following multiplicative Chernoff bound
%To continue our deriviation, we use the facts that $\forall\, k > \mu_X$ we have $\Prob{X \geq k} \leq e^{k-\mu_X}(\mu_X / k)^k$, and $\forall\, k \in (0,\mu_X]$% we have $\Prob{X \leq k} \leq e^{k-\mu_X}(\mu_X / k)^k$, when~$X$ is the sum of independent Poisson trials and $\mu_X = \mathbb{E}[X]$.
\begin{align*}
\Prob{\advA_1 \Rightarrow 0}  < &\; \Prob{\left. \sum_{i=1}^{\ell} X_i > \left(1+\frac{1}{6}\right)\frac{3\ell}{8} \;\right| m=0}\mdist(0) 
+ \Prob{\left. \sum_{i=1}^{\ell} X_i < \left(1-\frac{1}{10}\right)\frac{5\ell}{8} \;\right| m=1 }\mdist(1)\\
\leq &\; e^{-\frac{(3\ell/8)(1/6)^2}{3}}\mdist(0) + e^{-\frac{(5\ell/8)(1/10)^2}{2}}\mdist(1) \\
=&\; e^{-\frac{\ell}{288}}\mdist(0) + e^{-\frac{\ell}{320}}\mdist(1)
%\leq &\; e^{\frac{7\ell}{16}-\mu_0}\left(\frac{\mu_0}{7\ell/16}\right)^{\frac{7\ell}{16}}\mdist(0) 
%     \quad+\quad e^{\frac{9\ell}{16}-\mu_1}\left(\frac{\mu_1}{9\ell/16}\right)^{\frac{9\ell}{16}}\mdist(1)\\
%< &\; e^{\frac{7\ell}{16}-\frac{\ell}{4}}\left(\frac{3\ell/8}{7\ell/16}\right)^{\frac{7\ell}{16}}\mdist(0) 
%     \quad+\quad e^{\frac{9\ell}{16}-\frac{5\ell}{8}}\left(\frac{3\ell/4}{9\ell/16}\right)^{\frac{9\ell}{16}}\mdist(1)\\
%=  &\; e^{\frac{3\ell}{16}}\left(\frac{6}{7}\right)^{\frac{7\ell}{16}}\mdist(0) 
%     \quad+\quad e^{-\frac{\ell}{16}}\left(\frac{4}{3}\right)^{\frac{9\ell}{16}}\mdist(1)\\
\end{align*}
where 
%the second line follows (from the Chernoff bounds) because $\mu_0 \leq 3\ell/8 < 7\ell/16$ and $9\ell/16 < 5\ell/8 \leq \mu_1$; and 
\tsnote{If the lowerbound on the min-entropies $\beta_{i-1}$ is assumed to be larger, the bounds become better.  This would give a small improvement, here.}
This allows us to conclude that $\Prob{\advA_1 \Rightarrow 1 \given b=1} - \Prob{\advA_1 \Rightarrow 1 \given b=0}$ is at least \[
1- \left(e^{-\frac{\ell}{288}}\mdist(0) + e^{-\frac{\ell}{320}}\mdist(1) + 2e^{-\frac{\ell}{128}} \right)
\] 
when $\delta=1/8$.  For example, when $\ell=256$, the advantage of~$\advA_1$ is at least $1-0.412\mdist(0)-0.450\mdist(1)-0.271$, or roughly $0.3$ when $\mdist(0)=\mdist(1)=0.5$.
\tsnote{This bound is improved if you assume knowledge of~$m$ and use it to check just one ``side'', mostly because the factor of 2 in $2\exp(\ell/128)$ disappears.  But asymptotically, the results are the same.}
\end{proof}

\begin{corollary}(Informal.)\rm Assume that $\forall i\in[q]$ and $h_{i-1}$, 
the distributions $\mathcal{C}_{h_{i-1}}$ have min-entropy of at least one. 
When~$H\colon\bits^* \to \bits$ is a random oracle, the \texttt{One-Bit} scheme $(\enc^H,\dec^H)$ in
Figure~\ref{fig:one-bit} is not XXX-secure.  In particular, setting $\delta=1/8$,
adversary~$\advA_2$ achieves advantage at least $1 -
\left(e^{-\frac{\ell}{288}}\mdist(0) + e^{-\frac{\ell}{320}}\mdist(1)
\right)^{|\mathcal{K}|} - 1/c$ when $c>1$ is a constant such that
$\ell \geq 128\ln (2c|\mathcal{K}|)$.
\end{corollary}
\begin{proof}
   In the case that~$b=0$, we have $\Prob{\advA_2
    \Rightarrow 1} \leq |\mathcal{K}| 2e^{-\frac{\ell\delta^2}{2}}$ by
  a union bound and the proof of the previous theorem.  
In the case that~$b=1$, let $\mathrm{Key}_i$ be the
  event that key~$K_i$ causes~$\advA_1(c_1,c_2,\ldots,c_\ell,K_i)$ to
  output 1, for
  some enumeration $K_1,K_2,\ldots,K_{|\mathcal{K}|}$ of
  $\mathcal{K}$.
 \tsnote{The running time of the attack can
    be improved by ordering keys from most-to-least likely.}
  Consider $\Prob{\advA_2
    \Rightarrow 1}=1-\Prob{\advA_2 \Rightarrow 0}=1-\Prob{\bigwedge_{i}
    \neg\mathrm{Key_i}} = 1-\prod_i \Prob{\neg\mathrm{Key_i}}$, where
  independence follows because the keys are distinct and~$H$ is
  modeled as a random oracle.  Thus, again taking~$\delta=1/8$ we have
\[
\Prob{\advA_2 \Rightarrow 1}-\Prob{\advA_2 \Rightarrow 1} \geq 1 -
\left(e^{-\frac{\ell}{288}}\mdist(0) + e^{-\frac{\ell}{320}}\mdist(1)
\right)^{|\mathcal{K}|} - |\mathcal{K}| 2e^{-\frac{\ell}{128}}
\]
To ensure that the final term $|\mathcal{K}| 2e^{-\frac{\ell}{128}}
\leq 1/c$ for some constant~$c>1$, we require $\ell \geq 128\ln
(2c|\mathcal{K}|)$.  Moreover, the previous theorem tells us that when
$\ln(2c|\mathcal{K}|) > 2$, so that $\ell > 256$, the parenthesized
term vanishes as $|\mathcal{K}|$ grows.
\end{proof}

